{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Machine Learning\n",
    "\n",
    "# Home Exercise #1\n",
    "\n",
    "Assignment Due: 21/03/21 23:59\n",
    "\n",
    "Jonathan Schler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1Name=\"Alexander Kruglyak\" # student1 name here\n",
    "student1ID=\"323637736\" # student1 ID here\n",
    "\n",
    "\n",
    "student2Name=\"Sofya Zubtsovsky\" # student2 name here\n",
    "student2ID=\"337839112\" # student2 ID here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso, RidgeCV, LassoCV, ElasticNet\n",
    "from sklearn.impute import KNNImputer,SimpleImputer\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# add more imports as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Regression on Ames Housing Dataset (60 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the Ames housing dataset from <br>\n",
    "http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.xls <br>\n",
    "You can find a description of the variables here: <br>\n",
    "http://jse.amstat.org/v19n3/decock/DataDocumentation.txt <br>\n",
    "Take note that for categorical variables, NA here does not mean a missing value, but should be treated as a separate category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    return pd.read_excel(\"AmesHousing.xls\")\n",
    "df = read_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_FEATURES = [\n",
    "\"Mas Vnr Type\",\n",
    "\"Bsmt Qual\",\n",
    "\"Bsmt Cond\",\n",
    "\"Bsmt Exposure\",\n",
    "\"BsmtFin Type 1\",\n",
    "\"BsmtFin Type 2\",\n",
    "\"Electrical\",\n",
    "\"Garage Type\",\n",
    "\"Garage Finish\",\n",
    "\"Garage Qual\",\n",
    "\"Garage Cond\"\n",
    "]\n",
    "# \"Misc Feature\", \"Fence\", \"Pool QC\", \"Alley\" - because only ~200 rows contains data - less than 10%\n",
    "COLUMNS_TO_DROP = [\n",
    "    \"SalePrice\", \n",
    "    \"PID\", \n",
    "    \"Order\", \n",
    "    \"Misc Feature\", \n",
    "    \"Fence\", \n",
    "    \"Pool QC\", \n",
    "    \"Alley\", \n",
    "    \"Fireplace Qu\"\n",
    "] \n",
    "# + NA_FEATURES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, columns):\n",
    "    return df.copy().drop(columns, axis=1)\n",
    "\n",
    "\n",
    "dropped_df = drop_columns(df, COLUMNS_TO_DROP)\n",
    "continuous_features = dropped_df.select_dtypes(exclude=['object'])\n",
    "categorical_features = dropped_df.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1\n",
    "Visualize the univariate distribution of each continuous attribute, and the distribution of the target. Make sure you skip the string\\categorial columns (you can do it using call to `select_dtypes(exclude=['object'])`).\n",
    "\n",
    "Do you notice anything? Is there something that might require special treatment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: we need to know how to take the whole data\n",
    "y = df.SalePrice\n",
    "y_limit = y.max()\n",
    "y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_description = {\n",
    "    \"MS SubClass\": \"type of dwelling involved in the sale\",\n",
    "    \"Lot Frontage\": \"Linear feet of street connected to property\",\n",
    "    \"Lot Area\": \"Lot size in square feet\",\n",
    "    \"Overall Qual\": \"Rates the overall material and finish of the house\",\n",
    "    \"Overall Cond\": \"Rates the overall condition of the house\",\n",
    "    \"Year Built\": \"Original construction date\",\n",
    "    \"Year Remod/Add\": \"Remodel date (same as construction date if no remodeling or additions)\",\n",
    "    \"Mas Vnr Area\": \"Masonry veneer area in square feet\",\n",
    "    \"BsmtFin SF 1\": \"Type 1 finished square feet\",\n",
    "    \"BsmtFin SF 2\": \"Type 2 finished square feet\",\n",
    "    \"Bsmt Unf SF\": \"Unfinished square feet of basement area\",\n",
    "    \"Total Bsmt SF\": \"Total square feet of basement area\",\n",
    "    \"1st Flr SF\": \"First Floor square feet\",\n",
    "    \"2nd Flr SF\": \"Second floor square feet\",\n",
    "    \"Low Qual Fin SF\": \"Low quality finished square feet (all floors)\",\n",
    "    \"Gr Liv Area\": \"Above grade (ground) living area square feet\",\n",
    "    \"Bsmt Full Bath\": \"Basement full bathrooms\",\n",
    "    \"Bsmt Half Bath\": \"Basement half bathrooms\",\n",
    "    \"Bsmt Full Bath\": \"Basement full bathrooms\",\n",
    "    \"Full Bath\": \"Full bathrooms above grade\",\n",
    "    \"Half Bath\": \"Half baths above grade\",\n",
    "    \"Fireplaces\": \"Number of fireplaces\",\n",
    "    \"Misc Val\": \"Value of miscellaneous feature\",\n",
    "    \"3Ssn Porch\": \"Three season porch area in square feet\"\n",
    "}\n",
    "\n",
    "title_font = {\n",
    "        'color':  'darkblue',\n",
    "        'weight': 'normal',\n",
    "        'size': 20,\n",
    "        }\n",
    "labels_font = {\n",
    "        'color':  'darkred',\n",
    "        'weight': 'normal',\n",
    "        'size': 16,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(12, 3, figsize=(20, 70))\n",
    "\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    feature_name = continuous_features.columns[i]\n",
    "    feature_human_readable_name = features_description.get(feature_name, \"===NONE===\")\n",
    "    y = continuous_features[feature_name].value_counts().sort_index()\n",
    "    ax.set_title(\"{} / \\n {}\".format(feature_name, feature_human_readable_name), fontdict=title_font)\n",
    "    ax.set_ylabel(\"Counts\", fontdict=labels_font)\n",
    "    ax.plot(y)\n",
    "    ax.grid()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The plot allows to define the following:\n",
    "most of the feautures must be scaled and normalized:\n",
    "<ul>\n",
    "<li>lot frontage, lot area, total basement SF, garage year build and many other features aren't distributed around the median (there is a skew of the data)</li>\n",
    "<li>second floor, low quality SF, area and other features have a number of extreme values, and should be presented using log axis</li>\n",
    "<li>features have different scale, distribution and standard deviation</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2\n",
    "Visualize the dependency of the target on each continuous feature (2d scatter plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(12, 3, figsize=(20, 70))\n",
    "y = df.SalePrice\n",
    "\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    X_col = continuous_features.iloc[:, i]\n",
    "    ax.set_ylim(0, y_limit)\n",
    "    if X_col.nunique() > 10:\n",
    "        ax.set_xlim(X_col.quantile(q=.0001), X_col.quantile(q=.9999))\n",
    "        \n",
    "    ax.scatter(X_col, y, alpha=.5)\n",
    "\n",
    "    feature_name = continuous_features.columns[i]\n",
    "    feature_human_readable_name = features_description.get(feature_name, \"===NONE===\")\n",
    "    ax.set_title(\"{} / \\n {}\".format(feature_name, feature_human_readable_name), fontdict=title_font)\n",
    "    ax.set_ylabel(\"Price ($)\", fontdict=labels_font)\n",
    "    ax.grid()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3\n",
    "Split data in training and test set. Do not use the test-set unless for a final evaluation in 1.6.\n",
    "For each categorical variable, cross-validate a Linear Regression model using just this variable\n",
    "(one-hot-encoded). Visualize the relationship of the categorical variables that provide the best\n",
    "R^2 value with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot the price to see how it's spreaded across the indexes\n",
    "df.plot.line(y='SalePrice')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dropped_df\n",
    "#get dummies\n",
    "X_dummies = pd.get_dummies(X)\n",
    "#method 1 - split data in the ordinary way\n",
    "# what about stratify???\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dummies,y,random_state=0)\n",
    "\n",
    "#method 2 - split with stratification\n",
    "nof_bins = 50 #can be adjusted\n",
    "bins = np.linspace(np.min(y), len(y), nof_bins)\n",
    "# split continious SalePrice  into bins to make it 'categorical'\n",
    "y_binned = np.digitize(y, bins)\n",
    "X_stratified_train, X_stratified_test, y_stratified_train, y_stratified_test = train_test_split(X_dummies, y, stratify = y_binned,random_state=0)\n",
    "print ('median without stratify: train %f test %f\\nmedian with stratify: train %f, test %f' % (np.median(y_train), np.median(y_test),np.median(y_stratified_train), np.median(y_stratified_test)))\n",
    "\n",
    "#TBD check if the order between split and get_dummies is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: refactor to function\n",
    "for f in categorical_features.columns:\n",
    "    if categorical_features[f].isna().sum() /len(categorical_features[f]) > 0.3:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_features.plot(kind='box')\n",
    "#TBD add plot in a loop (doesn't work as expected)\n",
    "df.boxplot('SalePrice','Sale Condition',rot = 30,figsize=(5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# todo: consider better approach: with imputation\n",
    "# categorical_features = categorical_features.dropna()\n",
    "X_train, X_test, y_train, y_test = train_test_split(categorical_features, y, random_state=0)\n",
    "\n",
    "r2_scores = np.empty((0,2))\n",
    "for column in X_train:\n",
    "        two_d = X_train[column].values.reshape(-1,1)        \n",
    "        array_hot_encoded = OneHotEncoder(handle_unknown='ignore').fit_transform(two_d).toarray()\n",
    "        data_hot_encoded_df = pd.DataFrame(array_hot_encoded, index=X_train.index)\n",
    "        score = np.mean(cross_val_score(LinearRegression(), data_hot_encoded_df, y_train, cv=10))\n",
    "        r2_scores = np.concatenate((r2_scores, [[column, score]]))\n",
    "# r2_scores\n",
    "r2_scores = r2_scores[r2_scores[:,1].argsort()]\n",
    "r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[feature_name, score] = r2_scores[-1]\n",
    "feature_name\n",
    "data = categorical_features[feature_name]\n",
    "data\n",
    "# data.value_counts().plot(kind='bar')\n",
    "plt.scatter(data, y)\n",
    "plt.show()\n",
    "# ax = data.plot.bar(x=data, y=y_train, rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_DROP_COPY = COLUMNS_TO_DROP.copy()\n",
    "COLUMNS_TO_DROP_COPY.remove(\"SalePrice\")\n",
    "data = drop_columns(\n",
    "    read_data(),\n",
    "    COLUMNS_TO_DROP_COPY\n",
    ")\n",
    "\n",
    "# data[\"SalePrice\"]\n",
    "[feature_name, score] = r2_scores[-1]\n",
    "data.loc[dropped_df['Exter Qual'] == 'Gd']\n",
    "categories_within_feature = data[feature_name].unique()\n",
    "categories_within_feature\n",
    "\n",
    "prices_by_category = []\n",
    "for category in categories_within_feature:\n",
    "    # this will not work, we need to create in every array as much as categories variables\n",
    "    price_by_category = data[data[feature_name] == category][\"SalePrice\"].values\n",
    "#     print(price_by_category)\n",
    "    prices_by_category.append(price_by_category)\n",
    "    \n",
    "prices_by_category\n",
    "#     print(prices_by_category)\n",
    "# data = categorical_features[feature_name]\n",
    "price = np.random.rand(10, 4)\n",
    "price\n",
    "# categories_within_feature = data.unique()\n",
    "\n",
    "# df = pd.DataFrame(prices_by_category, columns=categories_within_feature)\n",
    "# df.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# categorical_features.index.tolist()\n",
    "fig, axes = plt.subplots(9, 3, figsize=(20, 70))\n",
    "y = df['SalePrice']\n",
    "print (len(y))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    f = categorical_features.columns.values[i]\n",
    "    print(f)\n",
    "    data = categorical_features[f]\n",
    "    # ax.scatter(data, y, alpha=.5)\n",
    "    categorical_features.boxplot(y,by=f,ax=ax)\n",
    "    # ax.boxplot(y,data)\n",
    "    ax.set_title(\"{}\".format(f), fontdict=title_font)\n",
    "    ax.set_ylabel(\"Price ($)\", fontdict=labels_font)\n",
    "    ax.grid()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.4\n",
    "Use ColumnTransformer and pipeline to encode categorical variables. Evaluate Linear\n",
    "Regression (OLS), Ridge, Lasso and ElasticNet using cross-validation with the default\n",
    "parameters. Does scaling the data (within the pipeline) with StandardScaler help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_numeric(df):\n",
    "    return pd.DataFrame(KNNImputer().fit_transform(df),columns = df.columns,index=df.index)\n",
    "\n",
    "numeric = dropped_df.select_dtypes(exclude=['object'])\n",
    "print  (\"before\\n\",numeric.isna().sum())\n",
    "numeric = impute_numeric(numeric)\n",
    "print (\"after\\n\",numeric.isna().sum())\n",
    "numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_categorical(df):\n",
    "    return df.fillna(\"missing\")\n",
    "categorical = dropped_df.select_dtypes(include=['object'])\n",
    "categorical = impute_categorical(categorical)\n",
    "categorical.isna().sum()\n",
    "categorical"
   ]
  },
  {
   "source": [
    "## Preprocess data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      MS SubClass  Lot Frontage  Lot Area  Overall Qual  Overall Cond  \\\n",
       "0            20.0         141.0   31770.0           6.0           5.0   \n",
       "1            20.0          80.0   11622.0           5.0           6.0   \n",
       "2            20.0          81.0   14267.0           6.0           6.0   \n",
       "3            20.0          93.0   11160.0           7.0           5.0   \n",
       "4            60.0          74.0   13830.0           5.0           5.0   \n",
       "...           ...           ...       ...           ...           ...   \n",
       "2925         80.0          37.0    7937.0           6.0           6.0   \n",
       "2926         20.0          79.0    8885.0           5.0           5.0   \n",
       "2927         85.0          62.0   10441.0           5.0           5.0   \n",
       "2928         20.0          77.0   10010.0           5.0           5.0   \n",
       "2929         60.0          74.0    9627.0           7.0           5.0   \n",
       "\n",
       "      Year Built  Year Remod/Add  Mas Vnr Area  BsmtFin SF 1  BsmtFin SF 2  \\\n",
       "0         1960.0          1960.0         112.0         639.0           0.0   \n",
       "1         1961.0          1961.0           0.0         468.0         144.0   \n",
       "2         1958.0          1958.0         108.0         923.0           0.0   \n",
       "3         1968.0          1968.0           0.0        1065.0           0.0   \n",
       "4         1997.0          1998.0           0.0         791.0           0.0   \n",
       "...          ...             ...           ...           ...           ...   \n",
       "2925      1984.0          1984.0           0.0         819.0           0.0   \n",
       "2926      1983.0          1983.0           0.0         301.0         324.0   \n",
       "2927      1992.0          1992.0           0.0         337.0           0.0   \n",
       "2928      1974.0          1975.0           0.0        1071.0         123.0   \n",
       "2929      1993.0          1994.0          94.0         758.0           0.0   \n",
       "\n",
       "      ...  Garage Area  Wood Deck SF  Open Porch SF  Enclosed Porch  \\\n",
       "0     ...        528.0         210.0           62.0             0.0   \n",
       "1     ...        730.0         140.0            0.0             0.0   \n",
       "2     ...        312.0         393.0           36.0             0.0   \n",
       "3     ...        522.0           0.0            0.0             0.0   \n",
       "4     ...        482.0         212.0           34.0             0.0   \n",
       "...   ...          ...           ...            ...             ...   \n",
       "2925  ...        588.0         120.0            0.0             0.0   \n",
       "2926  ...        484.0         164.0            0.0             0.0   \n",
       "2927  ...          0.0          80.0           32.0             0.0   \n",
       "2928  ...        418.0         240.0           38.0             0.0   \n",
       "2929  ...        650.0         190.0           48.0             0.0   \n",
       "\n",
       "      3Ssn Porch  Screen Porch  Pool Area  Misc Val  Mo Sold  Yr Sold  \n",
       "0            0.0           0.0        0.0       0.0      5.0   2010.0  \n",
       "1            0.0         120.0        0.0       0.0      6.0   2010.0  \n",
       "2            0.0           0.0        0.0   12500.0      6.0   2010.0  \n",
       "3            0.0           0.0        0.0       0.0      4.0   2010.0  \n",
       "4            0.0           0.0        0.0       0.0      3.0   2010.0  \n",
       "...          ...           ...        ...       ...      ...      ...  \n",
       "2925         0.0           0.0        0.0       0.0      3.0   2006.0  \n",
       "2926         0.0           0.0        0.0       0.0      6.0   2006.0  \n",
       "2927         0.0           0.0        0.0     700.0      7.0   2006.0  \n",
       "2928         0.0           0.0        0.0       0.0      4.0   2006.0  \n",
       "2929         0.0           0.0        0.0       0.0     11.0   2006.0  \n",
       "\n",
       "[2930 rows x 36 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MS SubClass</th>\n      <th>Lot Frontage</th>\n      <th>Lot Area</th>\n      <th>Overall Qual</th>\n      <th>Overall Cond</th>\n      <th>Year Built</th>\n      <th>Year Remod/Add</th>\n      <th>Mas Vnr Area</th>\n      <th>BsmtFin SF 1</th>\n      <th>BsmtFin SF 2</th>\n      <th>...</th>\n      <th>Garage Area</th>\n      <th>Wood Deck SF</th>\n      <th>Open Porch SF</th>\n      <th>Enclosed Porch</th>\n      <th>3Ssn Porch</th>\n      <th>Screen Porch</th>\n      <th>Pool Area</th>\n      <th>Misc Val</th>\n      <th>Mo Sold</th>\n      <th>Yr Sold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20.0</td>\n      <td>141.0</td>\n      <td>31770.0</td>\n      <td>6.0</td>\n      <td>5.0</td>\n      <td>1960.0</td>\n      <td>1960.0</td>\n      <td>112.0</td>\n      <td>639.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>528.0</td>\n      <td>210.0</td>\n      <td>62.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>2010.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.0</td>\n      <td>80.0</td>\n      <td>11622.0</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>1961.0</td>\n      <td>1961.0</td>\n      <td>0.0</td>\n      <td>468.0</td>\n      <td>144.0</td>\n      <td>...</td>\n      <td>730.0</td>\n      <td>140.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>120.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>2010.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20.0</td>\n      <td>81.0</td>\n      <td>14267.0</td>\n      <td>6.0</td>\n      <td>6.0</td>\n      <td>1958.0</td>\n      <td>1958.0</td>\n      <td>108.0</td>\n      <td>923.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>312.0</td>\n      <td>393.0</td>\n      <td>36.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>12500.0</td>\n      <td>6.0</td>\n      <td>2010.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20.0</td>\n      <td>93.0</td>\n      <td>11160.0</td>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>1968.0</td>\n      <td>1968.0</td>\n      <td>0.0</td>\n      <td>1065.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>522.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>2010.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>60.0</td>\n      <td>74.0</td>\n      <td>13830.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>1997.0</td>\n      <td>1998.0</td>\n      <td>0.0</td>\n      <td>791.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>482.0</td>\n      <td>212.0</td>\n      <td>34.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2010.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2925</th>\n      <td>80.0</td>\n      <td>37.0</td>\n      <td>7937.0</td>\n      <td>6.0</td>\n      <td>6.0</td>\n      <td>1984.0</td>\n      <td>1984.0</td>\n      <td>0.0</td>\n      <td>819.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>588.0</td>\n      <td>120.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2006.0</td>\n    </tr>\n    <tr>\n      <th>2926</th>\n      <td>20.0</td>\n      <td>79.0</td>\n      <td>8885.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>1983.0</td>\n      <td>1983.0</td>\n      <td>0.0</td>\n      <td>301.0</td>\n      <td>324.0</td>\n      <td>...</td>\n      <td>484.0</td>\n      <td>164.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>2006.0</td>\n    </tr>\n    <tr>\n      <th>2927</th>\n      <td>85.0</td>\n      <td>62.0</td>\n      <td>10441.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>1992.0</td>\n      <td>1992.0</td>\n      <td>0.0</td>\n      <td>337.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>80.0</td>\n      <td>32.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>700.0</td>\n      <td>7.0</td>\n      <td>2006.0</td>\n    </tr>\n    <tr>\n      <th>2928</th>\n      <td>20.0</td>\n      <td>77.0</td>\n      <td>10010.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>1974.0</td>\n      <td>1975.0</td>\n      <td>0.0</td>\n      <td>1071.0</td>\n      <td>123.0</td>\n      <td>...</td>\n      <td>418.0</td>\n      <td>240.0</td>\n      <td>38.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>2006.0</td>\n    </tr>\n    <tr>\n      <th>2929</th>\n      <td>60.0</td>\n      <td>74.0</td>\n      <td>9627.0</td>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>1993.0</td>\n      <td>1994.0</td>\n      <td>94.0</td>\n      <td>758.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>650.0</td>\n      <td>190.0</td>\n      <td>48.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>11.0</td>\n      <td>2006.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2930 rows × 36 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": [
    "numeric = impute_numeric(dropped_df.select_dtypes(exclude=['object']))\n",
    "categorical = impute_categorical(dropped_df.select_dtypes(include=['object']))\n",
    "\n",
    "#TBD consider how to fill columns of inplace: fill & KNNImputer does not fill in place when we pass a slice of dataframe\n",
    "# or think how to concatenate categorical/numeric columns after imputing\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_imputed, y, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "# preprocess_scaled = make_column_transformer(\n",
    "#     (StandardScaler(), ~categorical),\n",
    "#     (OneHotEncoder(), categorical)\n",
    "# )\n",
    "\n",
    "# preprocess_not_scaled = make_column_transformer(\n",
    "#     (OneHotEncoder(), categorical)\n",
    "# )\n",
    "\n",
    "# # disregard warning\n",
    "# rergressors = [LinearRegression(),Ridge(),Lasso(),ElasticNet()] \n",
    "# for regr in rergressors:\n",
    "#     non_caled_pipe = make_pipeline(preprocess_not_scaled,regr)\n",
    "#     scaled_pipe = make_pipeline(preprocess_scaled,regr)\n",
    "#     scores[regr] = (np.mean(cross_val_score(scaled_pipe, X_train, y_train)),np.mean(cross_val_score(non_caled_pipe, X_train, y_train)))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#take only categorical values from train set\n",
    "categorical = X_train.dtypes == object\n",
    "\n",
    "scores = {}\n",
    "preprocess = make_column_transformer(\n",
    "    (StandardScaler(), ~categorical),\n",
    "    (OneHotEncoder(), categorical))\n",
    "#disregard warning\n",
    "rergressors = [LinearRegression(),Ridge(),Lasso(),ElasticNet()] \n",
    "for regr in rergressors:\n",
    "    scaled_pipe = make_pipeline(KNNImputer(),preprocess,regr) #important to impute NaN values to avoid errors on regression\n",
    "    non_caled_pipe = make_pipeline(KNNImputer(),regr)\n",
    "    scores[regr] = (np.mean(cross_val_score(scaled_pipe, X_train, y_train)),np.mean(cross_val_score(non_caled_pipe, X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=scores,index=['scaled','non-scaled'],dtype=float)    \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.5\n",
    "Tune the parameters of the models using GridSearchCV. Do the results improve?\n",
    "Visualize the dependence of the validation score on the parameters for Ridge, Lasso and\n",
    "ElasticNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = X_train.dtypes == object\n",
    "preprocess = make_column_transformer(\n",
    "    (StandardScaler(), ~categorical),\n",
    "    (OneHotEncoder(), categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('imputer', KNNImputer()),\n",
    "                 ('scaler', preprocess),\n",
    "                 ('regressor', Ridge())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alpha_score(gridcv_results):\n",
    "    results = pd.DataFrame(gridcv_results)\n",
    "    results.plot('param_regressor__alpha', 'mean_train_score')\n",
    "    results.plot('param_regressor__alpha', 'mean_test_score', ax=plt.gca())\n",
    "    plt.fill_between(results.param_regressor__alpha.astype(np.float),\n",
    "                     results['mean_train_score'] + results['std_train_score'],\n",
    "                     results['mean_train_score'] - results['std_train_score'], alpha=0.2)\n",
    "    plt.fill_between(results.param_regressor__alpha.astype(np.float),\n",
    "                     results['mean_test_score'] + results['std_test_score'],\n",
    "                     results['mean_test_score'] - results['std_test_score'], alpha=0.2)\n",
    "    plt.legend()\n",
    "    plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alpha_l1_score(gridcv_results):\n",
    "    res = pd.pivot_table(pd.DataFrame(gridcv_results),\n",
    "    values='mean_test_score', index='param_regressor__alpha', columns='param_regressor__l1_ratio')\n",
    "    res = res.set_index(res.index.values.round(4))\n",
    "    sns.heatmap(res, annot=True, fmt=\".3g\", vmin=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBD decide if we need to use array of regressor instead of validate one by one\n",
    "# multiple_param_grid = [{'imputer': [KNNImputer()],\n",
    "#             'regressor': [Ridge()],\n",
    "#             'regressor__alpha': np.logspace(-3, 3, 13),\n",
    "#             'scaler': [preprocess]},\n",
    "#             {'imputer': [KNNImputer()],\n",
    "#              'regressor': [Lasso()],\n",
    "#             'regressor__alpha': np.logspace(-3, 0, 13),\n",
    "#             'scaler': [preprocess]},\n",
    "#             {'imputer': [KNNImputer()],\n",
    "#             'regressor': [ElasticNet()],\n",
    "#             'regressor__alpha': np.logspace(-4, -1, 10),\n",
    "#             'regressor__l1_ratio': [0.01, .1, .5, .9, .98, 1],\n",
    "#             'scaler': [preprocess]}\n",
    "#              ]\n",
    "# grid = GridSearchCV(pipe, multiple_param_grid,n_jobs=-1,return_train_score=True,cv=10)\n",
    "# grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIDGE\n",
    "ridge_param_grid = {'imputer': [KNNImputer()],\n",
    "            'regressor': [Ridge()],\n",
    "            'regressor__alpha': np.logspace(-3, 3, 13),\n",
    "            'scaler': [preprocess]}\n",
    "ridge_grid = GridSearchCV(pipe, ridge_param_grid,n_jobs=-1,return_train_score=True,cv=10)\n",
    "ridge_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_alpha_score(ridge_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO\n",
    "lasso_param_grid =  {'imputer': [KNNImputer()],\n",
    "             'regressor': [Lasso()],\n",
    "            'regressor__alpha': np.logspace(-3, 0, 13),\n",
    "            'scaler': [preprocess]}\n",
    "lasso_grid = GridSearchCV(pipe, lasso_param_grid,n_jobs=-1,return_train_score=True,cv=10)\n",
    "lasso_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_alpha_score(lasso_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELASTIC NET\n",
    "elastic_param_grid = {'imputer': [KNNImputer()],\n",
    "            'regressor': [ElasticNet()],\n",
    "            'regressor__alpha': np.logspace(-4, -1, 10),\n",
    "            'regressor__l1_ratio': [0.01, .1, .5, .9, .98, 1],\n",
    "            'scaler': [preprocess]}\n",
    "elastic_grid = GridSearchCV(pipe, elastic_param_grid,n_jobs=-1,return_train_score=True,cv=10)\n",
    "elastic_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_alpha_l1_score(elastic_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.6\n",
    "Visualize the coefficients of the resulting models. Do they agree on which features are\n",
    "important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = ridge_grid.best_estimator_.named_steps['regressor']\n",
    "lasso = lasso_grid.best_estimator_.named_steps['regressor']\n",
    "elastic = elastic_grid.best_estimator_.named_steps['regressor']\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.set_xlabel(\"Features\")\n",
    "ax.set_ylabel(\"Weight\")\n",
    "ax.plot(ridge.coef_, label=\"Ridge\",alpha=0.5)\n",
    "ax.plot(lasso.coef_, label=\"Lasso\",alpha=0.5)\n",
    "ax.plot(elastic.coef_, label=\"ElasticNet\",alpha=0.5)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in plot, there are number of features that got significantly different weights on regressors.\n",
    "We can try to analize what these features are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a closer look at 10 features with maximal standard deviation of weights among 3 regressors\n",
    "df= pd.DataFrame()\n",
    "df['Ridge']=ridge.coef_\n",
    "df['Lasso']=lasso.coef_\n",
    "df['ElasticNet']=elastic.coef_\n",
    "df['std'] = df.std(axis=1)\n",
    "df['Feature'] = X_train.columns\n",
    "df.sort_values('std',inplace=True,ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_divergent_features = df.loc[:10,['Ridge','Lasso','ElasticNet','Feature']]\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot()\n",
    "ax.set_title(\"10 most divergent features by weight\")\n",
    "ax.set_xlabel(\"Feature\")\n",
    "ax.set_ylabel(\"Weigth\")\n",
    "ax.legend(handles=df.loc[:10,'Feature'])\n",
    "most_divergent_features.plot(ax=ax,kind=\"bar\",legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Classification on the Telco-churn dataset (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the dataset and see it’s description at <br>\n",
    "https://www.kaggle.com/blastchar/telco-customer-churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "\n",
    "Load the data from the file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "\n",
    "df.head()\n",
    "# df.iloc[\"2229-VWQJH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "\n",
    "Split data into training and test set. Build a pipeline for dealing with categorical variables.\n",
    "Evaluate Logistic Regression and nearest centroids using\n",
    "cross-validation. How different are the results? How does scaling the continuous features with\n",
    "StandardScaler influence the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y = df.Churn\n",
    "x = df.drop([\"Churn\", \"customerID\"], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(pd.get_dummies(x), y, random_state=0)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "knn.fit(X_train, y_train)\n",
    "print(f\"accuracy: {knn.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "\n",
    "Tune the parameters using GridSearchCV. Do the results improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4\n",
    "\n",
    "Change the cross-validation strategy from ‘stratified k-fold’ to ‘kfold’ with shuffling. Do the\n",
    "parameters that are found change? Do they change if you change the random seed of the\n",
    "shuffling? Or if you change the random state of the split into training and test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}