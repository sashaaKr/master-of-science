{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Advanced Machine Learning\n",
    "\n",
    "# (Stochastic) Gradient Descent, Gradient Boosting and Imbalanced Data\n",
    "\n",
    "12/04/21\n",
    "\n",
    "Jonathan Schler\n",
    "\n",
    "(credit: Columbia Univ - 4995 - Machine Learning Course)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Gradient Descent\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/gradient_3d.png\" style=\"width: 600px;\">\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Want: $$\\arg \\min_w F(w)$$\n",
    "\n",
    "Initialize $w_0$\n",
    "\n",
    "$$w^{(i+1)} \\leftarrow w^{(i)} - \\eta_i\\frac{d}{dw}F(w^{(i)})$$\n",
    "\n",
    "Converges to local minimum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Gradient Descent\n",
    "\n",
    "$$w^{(i+1)} \\leftarrow w^{(i)} - \\eta_i\\frac{d}{dw}F(w^{(i)})$$\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/gradient_2d.png\" style=\"width: 600px;\">\n",
    "</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Linear Regression (reminder)\n",
    "$$\\hat{y} = w^T \\mathbf{x} + b = \\sum_{i=1}^p w_i x_i +b $$\n",
    "\n",
    "optimize SSE $$  \\min_{w \\in \\mathbb{R}^p, b\\in\\mathbb{R}} \\sum_{i=1}^n (w^T\\mathbf{x}_i + b - y_i)^2$$\n",
    "\n",
    "let's consider a simple case with just one variable\n",
    "\n",
    "$$\\min_{a \\in \\mathbb{R}, b\\in\\mathbb{R}} \\sum_{i=1}^n (a\\mathbf{x}_i + b - y_i)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Gradient Descent for Linear Regression\n",
    "\n",
    "1. Initialize the weights ($a$ & $b$) with random values and calculate Error ($SSE$)\n",
    "2. Calculate the gradient i.e. change in $SSE$ when the weights ($a$ & $b$) are changed by learning rate and derivative from their original randomly initialized value. \n",
    "3. Adjust the weights with the gradients to reach better values where $SSE$ is lower\n",
    "4. Use the new weights for prediction and to calculate the new $SSE$\n",
    "5. Repeat steps 2-4 till further adjustments to weights doesn’t significantly reduce the Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<center>\n",
    "<img src=\"./images/initial_housing_data2.png\" style=\"width: 600px;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 1\n",
    "\n",
    "Fit an initial equation $\\hat{y}$ = $a + b X$, start off with random values of $a$ and $b$ and calculate prediction error ($SSE$)\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/grad_desc_step1.png\" style=\"width: 600px;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 2\n",
    "\n",
    "Calculate the error gradient w.r.t the weights\n",
    "$$ SSE = \\frac{1}{2}\\sum_{i=1}^n (y_i - \\hat{y})^2 $$\n",
    "$$\\min_{a \\in \\mathbb{R}, b\\in\\mathbb{R}} \\frac{1}{2} \\sum_{i=1}^n ( y_i - (b\\mathbf{x}_i + a))^2$$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial SSE}{\\partial a} = -\\sum_{i=1}^n (y_i - b\\mathbf{x}_i - a ) = -\\sum_{i=1}^n (y_i - \\hat{y})  $$\n",
    "$$ \\frac{\\partial SSE}{\\partial b} = -\\sum_{i=1}^n (y_i - b\\mathbf{x}_i - a ) x_i = -\\sum_{i=1}^n (y_i - \\hat{y})x_i $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/grad_desc_step2.png\" style=\"width: 600px;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 3\n",
    "\n",
    "Adjust the weights with the gradients to reach better values where $SSE$ is lower\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/grad_desc_step3a.jpg\" style=\"width: 600px;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 3 - continued\n",
    "\n",
    "We need to update the random values of $a$,$b$ so that we move in the direction of optimal $a$, $b$.\n",
    "\n",
    "Update rules:\n",
    "\n",
    "   $$ a^{(i+1)} = a^i – \\frac{\\partial SSE}{\\partial a} $$\n",
    "   \n",
    "   \n",
    "   $$ b^{(i+1)} = b^i – \\frac{\\partial SSE}{\\partial b} $$\n",
    "\n",
    "So, update rules:\n",
    "\n",
    "$$ a^{2} = a^{1} – r * \\frac{∂SSE}{∂a} = 0.45-0.01*3.300 = 0.42 $$\n",
    "    \n",
    "$$ b^{2} = b^{1} – r * \\frac{∂SSE}{∂b} = 0.75-0.01*1.545 = 0.73 $$\n",
    "\n",
    "here, r is the learning rate = 0.01, which is the pace of adjustment to the weights.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 4\n",
    "\n",
    "Use new $a$ and $b$ for prediction and to calculate new Total $SSE$\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/grad_desc_step4.png\" style=\"width: 600px;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 5\n",
    "\n",
    "Repeat step $3$ and $4$ till the time further adjustments to $a$, $b$ doesn’t significantly reduces the error. At that time, we have arrived at the optimal $a$,$b$ with the highest prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Choosing a learning rate\n",
    "\n",
    "$$w^{(i+1)} \\leftarrow w^{(i)} - \\eta_i\\frac{d}{dw}F(w^{(i)})$$\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/gradient_learning_rates.png\" style=\"width: 600px;\">\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# (Stochastic) Gradient Descent\n",
    "\n",
    "Logistic Regression Objective:\n",
    "\n",
    "$$F(w, b) = -C \\sum_{i=1}^n\\log(\\exp(-y_iw^T \\textbf{x}_ii -b) +1 ) + ||w||_2^2$$\n",
    "\n",
    "Gradient:\n",
    "\n",
    "$$\\frac{d}{dw}F(w) = \\frac{d}{dw} -C \\sum_{i=1}^n\\log(\\exp(-y_iw^T \\textbf{x}_i - b) +1 ) + ||w||_2^2$$\n",
    "\n",
    "Stochastic Gradient: Pick $x_i$ randomly, then\n",
    "\n",
    "$$\\frac{d}{dw}F(w) \\approx \\frac{d}{dw} -C \\log(\\exp(-y_iw^T \\textbf{x}_ii -b) +1 ) + \\frac{1}{n}||w||_2^2$$\n",
    "\n",
    "In practice: just iterate over i.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# SGDClassifier, SGDRegressor and partial_fit\n",
    "```python\n",
    "# Run until convergence\n",
    "sgd = SGDClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Run one iteration over a batched dataset\n",
    "sgd = SGDClassifier()\n",
    "for X_batch, y_batch in batches:\n",
    "    sgd.partial_fit(X_batch, y_batch, classes=[0, 1, 2])\n",
    "\n",
    "# Run several iterations over a batched datasets\n",
    "for i in range(10):\n",
    "    for X_batch, y_batch in batches:\n",
    "        sgd.partial_fit(X_batch, y_batch, classes=[0, 1, 2])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# SGD and partial_fit\n",
    "\n",
    "- SGDClassifier(), SGDRegressor() fast on very large datasets\n",
    "- Tuning learning rate and schedule can be tricky.\n",
    "- partial_fit allows working with out-of-memory data!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Boosting \n",
    "\n",
    "$$f(x) = \\sum_k g_k(x)$$\n",
    "\n",
    " - Family of algorithms to create \"strong\" learner $f$ from \"weak\" learners $g_k$.\n",
    " - AdaBoost, GentleBoost, LogitBoost, …\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Gradient Boosting Algorithm\n",
    "\n",
    "\n",
    "\n",
    "$$ f_{1}(x) \\approx y  $$\n",
    "\n",
    "$$ f_{2}(x) \\approx y - f_{1}(x) $$\n",
    "\n",
    "$$ f_{3}(x) \\approx y - f_{1}(x) - f_{2}(x)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "$y \\approx$ ![:scale 22.5%](images/grad_boost_term_1.png) + ![:scale 22.5%](images/grad_boost_term_2.png) + ![:scale 20%](images/grad_boost_term_3.png) + ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Gradient Boosting Algorithm\n",
    "\n",
    "\n",
    "\n",
    "$$ f_{1}(x) \\approx y  $$\n",
    "\n",
    "$$ f_{2}(x) \\approx y - \\gamma f_{1}(x) $$\n",
    "\n",
    "$$ f_{3}(x) \\approx y - \\gamma f_{1}(x) - \\gamma f_{2}(x)$$\n",
    "\n",
    "\n",
    "$y \\approx \\gamma$ ![:scale 22.5%](images/grad_boost_term_1.png) + $\\gamma$ ![:scale 22.5%](images/grad_boost_term_2.png) + $\\gamma$ ![:scale 20%](images/grad_boost_term_3.png) + ...\n",
    "<br />\n",
    "<br />\n",
    "Learning rate $\\gamma, i.e. 0.1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Early stopping\n",
    "\n",
    "- Adding trees can lead to overfitting\n",
    "- Stop adding trees when validation accuracy stops increasing\n",
    "\n",
    "two choices:\n",
    "\n",
    "- pick number of trees and tune learning rate\n",
    "\n",
    "- pick learning rate, use early stopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Tuning of Gradient Boosting\n",
    "- Typically strong pruning via max_depth\n",
    "- Tune max_features\n",
    "- Tune column subsampling, row subsampling\n",
    "- Regularization\n",
    "- Pick learning rate and do early stopping\n",
    "- Or Pick n_estimators, tune learning rate (if not early stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Improvements:\n",
    "# \"extreme\" gradient boosting\n",
    "\n",
    "[XGBoost: A Scalable Tree Boosting System, 2016](http://dmlc.cs.washington.edu/data/pdf/XGBoostArxiv.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# XGBoost\n",
    "`conda install -c conda-forge xgboost`\n",
    "\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "- supports missing values\n",
    "- GPU training\n",
    "- networked parallel training\n",
    "- monotonicity constraints\n",
    "- supports sparse data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Gradient Boosting Advantages\n",
    "\n",
    "- Very fast using XGBoost (or HistGradientBoosting, LightGBM)\n",
    "- Small model size\n",
    "- Typically more accurate than Random Forests\n",
    "\n",
    "\n",
    "- \"old\" GradientBoosting in sklearn is comparatively slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Concluding tree-based models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "class:spacious\n",
    "# When to use tree-based models\n",
    "- Model non-linear relationships\n",
    "- Doesn’t care about scaling, no need for feature engineering\n",
    "- Single tree: very interpretable (if small)\n",
    "- Random forests very robust, good benchmark\n",
    "- Gradient boosting often best performance with careful tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calibration\n",
    "\n",
    "- Probabilities can be much more informative than labels:\n",
    "\n",
    "- “The model predicted you don’t have cancer” vs “The model predicted you’re 40% likely to have cancer”\n",
    "\n",
    "further reading can be found in this [article](http://www.datascienceassn.org/sites/default/files/Predicting%20good%20probabilities%20with%20supervised%20learning.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Calibration curve (Reliability diagram)\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/prob_table.png\" style=\"width: 600px;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "```python\n",
    "from sklearn.calibration import calibration_curve\n",
    "probs = lr.predict_proba(X_test)[:, 1]\n",
    "prob_true, prob_pred = calibration_curve(y_test, probs, n_bins=5)\n",
    "print(prob_true)\n",
    "print(prob_pred)\n",
    "[ 0.2    0.303  0.458  0.709  0.934]\n",
    "[ 0.138  0.306  0.498  0.701  0.926]\n",
    "```\n",
    "\n",
    "![:scale 70%](images/predprob_positive.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Influence of number of bins\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/influence_bins.png\" style=\"width: 600px;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comparing Models\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/calib_curve_models.png\" style=\"width: 600px;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fixing it: Calibrating a classifier\n",
    "- Build another model, mapping classifier probabilities to better probabilities!\n",
    "- 1d model! (or more for multi-class)\n",
    "\n",
    "$$ f_{calib}(s(x)) \\approx p(y)$$\n",
    "\n",
    "\n",
    "- s(x) is score given by model, usually\n",
    "- Can also work with models that don’t even provide probabilities!\n",
    "Need model for $f_{calib}$, need to decide what data to train it on.\n",
    "- Can train on training set → Overfit\n",
    "- Can train using cross-validation → use data, slower\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Platt Scaling\n",
    "\n",
    "- Use a logistic sigmoid for $f_{calib}$\n",
    "- Basically learning a 1d logistic regression\n",
    "- (+ some tricks)\n",
    "- Works well for SVMs\n",
    "\n",
    "$$f_{platt} = \\frac{1}{1 + exp(-ws(x) - b)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Isotonic Regression\n",
    "\n",
    "- Very flexible way to specify $f_{calib}$\n",
    "- Learns arbitrary monotonically increasing step-functions in 1d.\n",
    "- Groups data into constant parts, steps in between.\n",
    "- Optimum monotone function on training data (wrt mse).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/isotonic_regression.png\" style=\"width: 600px;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building the model\n",
    "- Using the training set is bad\n",
    "- Either use hold-out set or cross-validation\n",
    "- Cross-validation can be used to make unbiased probability predictions, use that as training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Fitting the calibration model\n",
    "<center>\n",
    "<img src=\"./images/calibration_val_scores.png\" style=\"width: 600px;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Fitting the calibration model\n",
    "<center>\n",
    "<img src=\"./images/calibration_val_scores_fitted.png\" style=\"width: 600px;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# CalibratedClassifierCV\n",
    "\n",
    "```python\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train,\n",
    "                                                          stratify=y_train, random_state=0)\n",
    "rf = RandomForestClassifier().fit(X_train_sub, y_train_sub)\n",
    "scores = rf.predict_proba(X_test)[:, 1]\n",
    "plot_calibration_curve(y_test, scores, n_bins=20)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/random_forest.png\" style=\"width: 600px;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Calibration on Random Forest\n",
    "\n",
    "```python\n",
    "cal_rf = CalibratedClassifierCV(rf, cv=\"prefit\", method='sigmoid')\n",
    "cal_rf.fit(X_val, y_val)\n",
    "scores_sigm = cal_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "cal_rf_iso = CalibratedClassifierCV(rf, cv=\"prefit\", method='isotonic')\n",
    "cal_rf_iso.fit(X_val, y_val)\n",
    "scores_iso = cal_rf_iso.predict_proba(X_test)[:, 1]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/types_calib.png\" style=\"width: 600px;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross-validated Calibration\n",
    "\n",
    "```python\n",
    "cal_rf_iso_cv = CalibratedClassifierCV(rf, method='isotonic')\n",
    "cal_rf_iso_cv.fit(X_train, y_train)\n",
    "scores_iso_cv = cal_rf_iso_cv.predict_proba(X_test)[:, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/types_calib_cv.png\" style=\"width: 600px;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Two sources of imbalance\n",
    "\n",
    "- Asymmetric cost\n",
    "- Asymmetric data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why do we care?\n",
    "\n",
    "- Why should cost be symmetric?\n",
    "- All data is imbalanced\n",
    "- Detect rare events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91        53\n",
      "           1       0.95      0.96      0.95        90\n",
      "\n",
      "    accuracy                           0.94       143\n",
      "   macro avg       0.93      0.93      0.93       143\n",
      "weighted avg       0.94      0.94      0.94       143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathan.schler/anaconda3/envs/python37/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#first method - what can we do here?\n",
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, stratify=data.target, random_state=0)\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.98      0.90        53\n",
      "           1       0.99      0.88      0.93        90\n",
      "\n",
      "    accuracy                           0.92       143\n",
      "   macro avg       0.91      0.93      0.91       143\n",
      "weighted avg       0.93      0.92      0.92       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict_proba(X_test)[:, 1] > .85\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11183, 6)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "# mammography https://www.openml.org/d/310\n",
    "data = fetch_openml('mammography', as_frame=True)\n",
    "X, y = data.data, data.target\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    10923\n",
       "1       260\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# make y boolean\n",
    "# this allows sklearn to determine the positive class more easily\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y == '1', random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.918891506439645, 0.6414186442735843)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_validate(LogisticRegression(),\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9397708663466044, 0.7401040166316957)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "scores = cross_validate(RandomForestClassifier(),\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Basic Approaches\n",
    "\n",
    "- change the data\n",
    "  - add samples\n",
    "  - remove samples\n",
    "  - both\n",
    "- change the training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Imbalance-Learn\n",
    "\n",
    "http://imbalanced-learn.org\n",
    "\n",
    "```\n",
    "pip install -U imbalanced-learn\n",
    "```\n",
    "\n",
    "Extends `sklearn` API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sampler\n",
    "\n",
    "- To resample a data sets, each sampler implements `sample` method\n",
    "```python\n",
    "  data_resampled, targets_resampled = obj.sample(data, targets)\n",
    "```\n",
    "\n",
    "- Fitting and sampling can also be done in one step, using `fit_resample`\n",
    "\n",
    "```python\n",
    "  data_resampled, targets_resampled = obj.fit_resample(data, targets)\n",
    "```\n",
    "\n",
    "-  In Pipelines: Sampling only done in fit!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8387, 6)\n",
      "(392, 6)\n",
      "[196 196]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import numpy as np\n",
    "\n",
    "rus = RandomUnderSampler(replacement=False)\n",
    "X_train_subsample, y_train_subsample = rus.fit_resample(\n",
    "    X_train, y_train)\n",
    "print(X_train.shape)\n",
    "print(X_train_subsample.shape)\n",
    "print(np.bincount(y_train_subsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9102987534502297, 0.5708443138695001)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.pipeline import make_pipeline as make_imb_pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "undersample_pipe = make_imb_pipeline(RandomUnderSampler(), LogisticRegressionCV())\n",
    "scores = cross_validate(undersample_pipe,\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "# baseline was 0.9180, 0.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9456055985805664, 0.6341404950168421)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "undersample_pipe_rf = make_imb_pipeline(RandomUnderSampler(),\n",
    "                                        RandomForestClassifier())\n",
    "scores = cross_validate(undersample_pipe_rf,\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "# baseline was 0.939, 0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8387, 6)\n",
      "(16382, 6)\n",
      "[8191 8191]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "X_train_oversample, y_train_oversample = ros.fit_resample(\n",
    "    X_train, y_train)\n",
    "print(X_train.shape)\n",
    "print(X_train_oversample.shape)\n",
    "print(np.bincount(y_train_oversample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9234420400594983, 0.5577447362579694)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversample_pipe = make_imb_pipeline(RandomOverSampler(), LogisticRegression())\n",
    "scores = cross_validate(oversample_pipe,\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "# baseline was 0.920, 0.630\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9182500536833013, 0.7061310475947156)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversample_pipe_rf = make_imb_pipeline(RandomOverSampler(),\n",
    "                                       RandomForestClassifier())\n",
    "scores = cross_validate(oversample_pipe_rf,\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "# baseline was 0.939, 0.722\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class-weights\n",
    "\n",
    "- Instead of repeating samples, re-weight the loss function.\n",
    "- Works for most models!\n",
    "- Same effect as over-sampling (though not random), but not as expensive (dataset size the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class-weights in linear models\n",
    "\n",
    "$$\\min_{w \\in ℝ^{p}, b \\in \\mathbb{R}}-C \\sum_{i=1}^n\\log(\\exp(-y_i(w^T \\textbf{x}_i + b )) + 1) + ||w||_2^2$$\n",
    "\n",
    "$$\\min_{w \\in ℝ^{p}, b \\in \\mathbb{R}}-C \\sum_{i=1}^n c_{y_i}  \\log(\\exp(-y_i(w^T \\textbf{x}_i + b )) + 1) + ||w||_2^2$$\n",
    "\n",
    "Similar for linear and non-linear SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9235896730569388, 0.5644209092265684)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validate(LogisticRegression(class_weight='balanced'),\n",
    "#scores = cross_validate(LogisticRegression(class_weight=[1,100]),\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "# baseline was 0.920, 0.630\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9180817310631173, 0.7046508711873969)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validate(RandomForestClassifier(n_estimators=100,\n",
    "                                               class_weight='balanced'),\n",
    "                        X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()\n",
    "# baseline was 0.939, 0.722\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Synthetic Sample Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Synthetic Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "- Adds synthetic interpolated data to smaller class\n",
    "- For each sample in minority class:</br>\n",
    "    - Pick random neighbor from k neighbors.</br>\n",
    "    - Pick point on line connecting the two uniformly (or within rectangle)</br>\n",
    "    - Repeat.</br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/smote_mammography.png\" style=\"width: 600px;\">\n",
    "</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_roc_auc              0.921647\n",
       "test_average_precision    0.559659\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "smote_pipe = make_imb_pipeline(SMOTE(), LogisticRegression())\n",
    "scores = cross_validate(smote_pipe, X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "pd.DataFrame(scores)[['test_roc_auc', 'test_average_precision']].mean()\n",
    "# baseline was 0.920, 0.630\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_roc_auc              0.938048\n",
       "test_average_precision    0.707201\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smote_pipe_rf = make_imb_pipeline(SMOTE(),\n",
    "                                  RandomForestClassifier())\n",
    "scores = cross_validate(smote_pipe_rf, X_train, y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "pd.DataFrame(scores)[['test_roc_auc', 'test_average_precision']].mean()\n",
    "# baseline was 0.939, 0.722\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The results are pretty similar to either the original dataset or the random sampling. Performing nearest neighbors, I found 11 to be best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a21842c90>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'smote__k_neighbors': [3, 5, 7, 9, 11, 15, 31]}\n",
    "search = GridSearchCV(smote_pipe_rf, param_grid, cv=10,\n",
    "                      scoring=\"average_precision\",return_train_score=True)\n",
    "search.fit(X_train, y_train)\n",
    "results = pd.DataFrame(search.cv_results_)\n",
    "results.plot(\"param_smote__k_neighbors\", [\"mean_test_score\", \"mean_train_score\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/smote_k_neighbors.png\" style=\"width: 600px;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- calibration can help tuning the model and prediction\n",
    "- Always check roc_auc and average_precision look at curves\n",
    "- Undersampling is very fast and can help!\n",
    "- Undersampling + Ensembles is very powerful!\n",
    "- Can add synthetic samples with SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
