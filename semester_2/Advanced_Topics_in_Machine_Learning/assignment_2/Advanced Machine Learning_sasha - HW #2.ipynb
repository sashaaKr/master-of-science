{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Machine Learning\n",
    "\n",
    "# Home Exercise #2\n",
    "\n",
    "Assignment Due: 18/04/21 23:59\n",
    "\n",
    "Jonathan Schler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student1Name=\"Alexander Kruglyak\" # student1 name here\n",
    "student1ID=\"323637736\" # student1 ID here\n",
    "\n",
    "\n",
    "student2Name=\"Sofya Zubtsovsky\" # student2 name here\n",
    "student2ID=\"337839112\" # student2 ID here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this homework is to provide a realistic setting for a machine learning task. Therefore instructions will not specify the exact steps to carry out. Instead, it is part of theassignment to identify promising features, models and preprocessing methods and apply themas appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall goal is to predict whether a specific credit card transaction is fraudulent or not. Please use the data from this dataset provided on Kaggle here:https://www.kaggle.com/kartik2112/fraud-detection. The Kaggle page also contains a description of the dataset. You will use the train set for training the model and test set for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are free to manipulate the train dataset as you think is appropriate. You do not have to use the wholedataset, you can subsample if you want. While the modelling process is likely iterative, please lay out the  following tasks in the given order to facilitate grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_DROP = [\n",
    "#     \"Unnamed: 0\",\n",
    "#     \"trans_num\",\n",
    "#     \"trans_date_trans_time\", # TODO: we need to split it to bins of day\n",
    "    \n",
    "#     \"first\", \"last\", # TODO: we can think to identify nationality by name\n",
    "] \n",
    "\n",
    "def get_continuous_features(df):\n",
    "    return df.select_dtypes(exclude=['object'])\n",
    "\n",
    "def get_categorical_features(df):\n",
    "    return df.select_dtypes(include=['object'])\n",
    "\n",
    "def read_test_data():\n",
    "    return drop_columns(read_data(\"fraudTest.csv\"), COLUMNS_TO_DROP)\n",
    "\n",
    "def read_data(file_name):\n",
    "    return pd.read_csv(file_name)\n",
    "\n",
    "def drop_columns(df, columns):\n",
    "    return df.copy().drop(columns, axis=1,errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_description = {\n",
    "'transdatetrans_time' : 'Transaction DateTime',\n",
    "'cc_num' : 'Credit Card Number of Customer',\n",
    "'merchant' : 'Merchant Name',\n",
    "'category' : 'Category of Merchant',\n",
    "'amt' : 'Amount of Transaction',\n",
    "'first' : 'First Name of Credit Card Holder',\n",
    "'last' : 'Last Name of Credit Card Holder',\n",
    "'gender' : 'Gender of Credit Card Holder',\n",
    "'street' : 'Street Address of Credit Card Holder',\n",
    "'city' : 'City of Credit Card Holder',\n",
    "'state' : 'State of Credit Card Holder',\n",
    "'zip' : 'Zip of Credit Card Holder',\n",
    "'lat' : 'Latitude Location of Credit Card Holder',\n",
    "'long' : 'Longitude Location of Credit Card Holder',\n",
    "'city_pop' : 'Credit Card Holder City Population',\n",
    "'job' : 'Job of Credit Card Holder',\n",
    "'dob' :'Date of Birth of Credit Card Holder',\n",
    "'trans_num' : 'Transaction Number',\n",
    "'unix_time' : 'UNIX Time of transaction',\n",
    "'merch_lat' : 'Latitude Location of Merchant',\n",
    "'merch_long' : 'Longitude Location of Merchant',\n",
    "'is_fraud' : 'Fraud Flag'\n",
    "}\n",
    "\n",
    "title_font = {\n",
    "        'color':  'darkblue',\n",
    "        'weight': 'normal',\n",
    "        'size': 20,\n",
    "        }\n",
    "labels_font = {\n",
    "        'color':  'darkred',\n",
    "        'weight': 'normal',\n",
    "        'size': 16,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### trans_date_trans_time to trans_date_category ####\n",
    "\n",
    "def get_hour_as_category(hour):\n",
    "        day_time_category = \"night\"\n",
    "        \n",
    "        if 6 <= hour and hour < 12: \n",
    "            day_time_category = \"morning\"\n",
    "        elif hour >=12 and hour < 18:\n",
    "            day_time_category = \"afternoon\"\n",
    "        elif hour >= 18 and hour < 22:\n",
    "            day_time_category = \"evening\"\n",
    "            \n",
    "        return day_time_category\n",
    "\n",
    "def convert_to_categorical_trans_date(df):\n",
    "    _df = df.copy()\n",
    "    trans_date_category_list = []\n",
    "    \n",
    "    for  i, d in enumerate(get_categorical_features(_df).trans_date_trans_time):\n",
    "        day_of_week = datetime.datetime.strptime(d, '%Y-%m-%d %H:%M:%S').strftime('%A');\n",
    "        hour = datetime.datetime.strptime(d, '%Y-%m-%d %H:%M:%S').hour\n",
    "        trans_date_category = \"{}_{}\".format(day_of_week, hour)\n",
    "        trans_date_category_list.append(trans_date_category)\n",
    "    \n",
    "    _df['trans_date_category'] = trans_date_category_list\n",
    "    return _df\n",
    "\n",
    "def convert_to_categorical_trans_date_and_drop(df):\n",
    "    converted_df = convert_to_categorical_trans_date(df)\n",
    "    return drop_columns(converted_df, [\"trans_date_trans_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### city_pop to city_pop_category ####\n",
    "\n",
    "def population_as_category(p):\n",
    "    category = \"2M+\"\n",
    "    \n",
    "    if p <= 100:\n",
    "        category = \"100-\"\n",
    "    elif p <= 500:\n",
    "        category = \"500-\"\n",
    "    elif p <= 1000:\n",
    "        category = \"1K-\"\n",
    "    elif p <= 2000:\n",
    "        category = \"2K-\"\n",
    "    elif p <= 3000:\n",
    "        category = \"3K-\"\n",
    "    elif p <= 5000:\n",
    "        category = \"5K-\"\n",
    "    elif p <= 10000:\n",
    "        category = \"10K-\"\n",
    "    elif p <= 30000:\n",
    "        category = \"30K-\"\n",
    "    elif p <= 50000:\n",
    "        category = \"50K-\"\n",
    "    elif p <= 100000:\n",
    "        category = \"100K-\"\n",
    "    elif p <= 200000:\n",
    "        category = \"200K-\"\n",
    "    elif p <= 500000:\n",
    "        category = \"500K-\"\n",
    "    elif p <= 1000000:\n",
    "        category = \"1M-\"\n",
    "    elif p <= 1500000:\n",
    "        category = \"1.5M-\"\n",
    "    elif p <= 2000000:\n",
    "        category = \"2M-\"\n",
    "        \n",
    "    return category\n",
    "\n",
    "def convert_to_categorical_city_pop(df):\n",
    "    _df = df.copy()\n",
    "    city_pop_category_list = []\n",
    "    \n",
    "    for p in df.city_pop:\n",
    "        category = population_as_category(p)\n",
    "        city_pop_category_list.append(category)\n",
    "        \n",
    "    _df['city_pop_category'] = city_pop_category_list\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cc_to_cc_type_and_drop(df):\n",
    "    _df = convert_cc_to_cc_type(df)\n",
    "    return drop_columns(_df, [\"cc_num\"])\n",
    "\n",
    "def convert_cc_to_cc_type(df):\n",
    "    _df = df.copy()\n",
    "    cc_category_list = []\n",
    "    \n",
    "    for cc in df.cc_num:\n",
    "        cc_category_list.append(get_cc_type(cc))\n",
    "        \n",
    "    _df['cc_type'] = cc_category_list\n",
    "    return _df\n",
    "\n",
    "def get_cc_type(cc_number):\n",
    "    cc_number = str(cc_number)\n",
    "    if is_american_express(cc_number): return 'AMEX'\n",
    "    if is_visa(cc_number): return 'VISA'\n",
    "    if is_mastercard(cc_number): return 'MasterCard'\n",
    "    if is_discover(cc_number): return 'Discover'\n",
    "    if is_jcb(cc_number): return 'JCB'\n",
    "    if is_diners_club(cc_number): return 'DinersClub'\n",
    "    if is_laser(cc_number): return 'Laser'\n",
    "    if is_maestro(cc_number): return 'Maestro'\n",
    "    if is_visa_electron(cc_number): return 'VisaElectron'\n",
    "    if is_total_rewards_visa(cc_number): return 'TotalRewardsVisa'\n",
    "    if is_diners_club_carte_blanche(cc_number): return 'DinersClubCarteBlanche'\n",
    "    if is_diners_club_carte_international(cc_number): return 'DinersClubCarteInt'\n",
    "    \n",
    "    return 'unknown'\n",
    "\n",
    "def is_american_express(cc_number):\n",
    "    \"\"\"Checks if the card is an american express. If us billing address country code, & is_amex, use vpos\n",
    "    https://en.wikipedia.org/wiki/Bank_card_number#cite_note-GenCardFeatures-3\n",
    "    :param cc_number: unicode card number\n",
    "    \"\"\"\n",
    "    return bool(re.match(r'^3[47][0-9]{13}$', cc_number))\n",
    "\n",
    "\n",
    "def is_visa(cc_number):\n",
    "    \"\"\"Checks if the card is a visa, begins with 4 and 12 or 15 additional digits.\n",
    "    :param cc_number: unicode card number\n",
    "    \"\"\"\n",
    "\n",
    "    # Standard Visa is 13 or 16, debit can be 19\n",
    "    if bool(re.match(r'^4', cc_number)) and len(cc_number) in [13, 16, 19]:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_mastercard(cc_number):\n",
    "    \"\"\"Checks if the card is a mastercard. Begins with 51-55 or 2221-2720 and 16 in length.\n",
    "    :param cc_number: unicode card number\n",
    "    \"\"\"\n",
    "    if len(cc_number) == 16 and cc_number.isdigit():  # Check digit, before cast to int\n",
    "        return bool(re.match(r'^5[1-5]', cc_number)) or int(cc_number[:4]) in range(2221, 2721)\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_discover(cc_number):\n",
    "    \"\"\"Checks if the card is discover, re would be too hard to maintain. Not a supported card.\n",
    "    :param cc_number: unicode card number\n",
    "    \"\"\"\n",
    "    if len(cc_number) == 16:\n",
    "        try:\n",
    "            # return bool(cc_number[:4] == '6011' or cc_number[:2] == '65' or cc_number[:6] in range(622126, 622926))\n",
    "            return bool(cc_number[:4] == '6011' or cc_number[:2] == '65' or 622126 <= int(cc_number[:6]) <= 622925)\n",
    "        except ValueError:\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_jcb(cc_number):\n",
    "    \"\"\"Checks if the card is a jcb. Not a supported card.\n",
    "    :param cc_number: unicode card number\n",
    "    \"\"\"\n",
    "    # return bool(re.match(r'^(?:2131|1800|35\\d{3})\\d{11}$', cc_number))  # wikipedia\n",
    "    return bool(re.match(r'^35(2[89]|[3-8][0-9])[0-9]{12}$', cc_number))  # PawelDecowski\n",
    "\n",
    "\n",
    "def is_diners_club(cc_number):\n",
    "    \"\"\"Checks if the card is a diners club. Not a supported card.\n",
    "    :param cc_number: unicode card number\n",
    "    \"\"\"\n",
    "    return bool(re.match(r'^3(?:0[0-6]|[68][0-9])[0-9]{11}$', cc_number))  # 0-5 = carte blance, 6 = international\n",
    "\n",
    "\n",
    "def is_laser(cc_number):\n",
    "    \"\"\"Checks if the card is laser. Not a supported card.\n",
    "    :param cc_number: unicode card number\n",
    "    \"\"\"\n",
    "    return bool(re.match(r'^(6304|670[69]|6771)', cc_number))\n",
    "\n",
    "\n",
    "def is_maestro(cc_number):\n",
    "    \"\"\"Checks if the card is maestro. Not a supported card.\n",
    "    :param cc_number: unicode card number\n",
    "    \"\"\"\n",
    "    possible_lengths = [12, 13, 14, 15, 16, 17, 18, 19]\n",
    "    return bool(re.match(r'^(50|5[6-9]|6[0-9])', cc_number)) and len(cc_number) in possible_lengths\n",
    "\n",
    "\n",
    "# Child cards\n",
    "\n",
    "def is_visa_electron(cc_number):\n",
    "    \"\"\"Child of visa. Checks if the card is a visa electron. Not a supported card.\n",
    "    :param cc_number: unicode card number\n",
    "    \"\"\"\n",
    "    return bool(re.match(r'^(4026|417500|4508|4844|491(3|7))', cc_number)) and len(cc_number) == 16\n",
    "\n",
    "\n",
    "def is_total_rewards_visa(cc_number):\n",
    "    \"\"\"Child of visa. Checks if the card is a Total Rewards Visa. Not a supported card.\n",
    "    :param cc_number: unicode card number\n",
    "    \"\"\"\n",
    "    return bool(re.match(r'^41277777[0-9]{8}$', cc_number))\n",
    "\n",
    "\n",
    "def is_diners_club_carte_blanche(cc_number):\n",
    "    \"\"\"Child card of diners. Checks if the card is a diners club carte blance. Not a supported card.\n",
    "    :param cc_number: unicode card number\n",
    "    \"\"\"\n",
    "    return bool(re.match(r'^30[0-5][0-9]{11}$', cc_number))  # github PawelDecowski, jquery-creditcardvalidator\n",
    "\n",
    "\n",
    "def is_diners_club_carte_international(cc_number):\n",
    "    \"\"\"Child card of diners. Checks if the card is a diners club international. Not a supported card.\n",
    "    :param cc_number: unicode card number\n",
    "    \"\"\"\n",
    "    return bool(re.match(r'^36[0-9]{12}$', cc_number))  # jquery-creditcardvalidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partial_data(df, percent_of_data):\n",
    "    return df.sample(frac=percent_of_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dob_to_age(df): # calculate age in days at the moment of transaction\n",
    "    _df = df.copy()\n",
    "    age_in_days = []\n",
    "    \n",
    "    for index, row in _df.iterrows():\n",
    "        birth_date  = datetime.datetime.strptime(row['dob'], '%Y-%m-%d')\n",
    "        trans_date  = datetime.datetime.strptime(row['trans_date_trans_time'],'%Y-%m-%d %H:%M:%S')\n",
    "        age = trans_date - birth_date\n",
    "        age_in_days.append(age.days)\n",
    "    \n",
    "    _df['age_in_days'] = age_in_days\n",
    "    return _df\n",
    "\n",
    "def convert_dob_to_age_and_drop(df):\n",
    "    converted_df = convert_dob_to_age(df)\n",
    "    return drop_columns(converted_df, [\"dob\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unix_to_hour(df):\n",
    "    _df = df.copy()\n",
    "    _df['unix_time'] = _df['unix_time']  % 86400 #(86400s per 24h)\n",
    "    _df['unix_time'] /= (86400/24)\n",
    "    _df['trans_hour'] = _df['unix_time']\n",
    "    _df = drop_columns(_df,'unix_time')\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unite_state_city(df):\n",
    "    _df = df.copy()\n",
    "    state_city_list = []\n",
    "    \n",
    "    for index, row in _df.iterrows():\n",
    "        state_city = row['state'] + \"_\" + row['city']\n",
    "        state_city_list.append(state_city)\n",
    "        \n",
    "    _df['city'] = state_city_list\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_scale(_df):\n",
    "    df = _df.copy()\n",
    "    \n",
    "    rob_scaler = RobustScaler()\n",
    "\n",
    "    df['scaled_amount'] = rob_scaler.fit_transform(df['amt'].values.reshape(-1,1))\n",
    "\n",
    "    scaled_amount = df['scaled_amount']\n",
    "\n",
    "    df.drop(['scaled_amount', 'amt'], axis=1, inplace=True)\n",
    "    df.insert(0, 'scaled_amount', scaled_amount)\n",
    "    \n",
    "    \n",
    "    converted_df = convert_cc_to_cc_type_and_drop(df)\n",
    "    print(\"convert_cc_to_cc_type_and_drop finished\")\n",
    "    converted_df = convert_dob_to_age_and_drop(converted_df)\n",
    "    print(\"convert_dob_to_age_and_drop finished\")\n",
    "    converted_df = unite_state_city(converted_df)\n",
    "    print(\"unite_state_city finished\")\n",
    "    converted_df = convert_unix_to_hour(converted_df)\n",
    "    print(\"convert_unix_to_hour finished\")\n",
    "\n",
    "    converted_df['category'] = converted_df['category'].astype('category').cat.codes\n",
    "    converted_df['city'] = converted_df['city'].astype('category').cat.codes\n",
    "    converted_df['cc_type'] = converted_df['cc_type'].astype('category').cat.codes\n",
    "    converted_df['gender'] = converted_df['gender'].astype('category').cat.codes\n",
    "    converted_df['merchant'] = converted_df['merchant'].astype('category').cat.codes\n",
    "    converted_df['state']=converted_df['state'].astype('category').cat.codes\n",
    "    converted_df['job']=converted_df['job'].astype('category').cat.codes\n",
    "\n",
    "    return converted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Identify Features\n",
    "Assemble a dataset consisting of features and target (for example in a dataframe or in two arrays X and y). What features are relevant for the prediction task? What features should be excluded because they leak the target information? Show visualizations or statistics to support your selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df = pd.read_csv(\"fraudTrain.csv\")\n",
    "full_test_df = pd.read_csv(\"fraudTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### T R A I N data\n",
    "X_train = full_train_df.copy()\n",
    "Y_train = full_train_df['is_fraud']\n",
    "# del X_train['is_fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### T E S T data\n",
    "X_test = full_test_df.copy()\n",
    "Y_test = full_test_df['is_fraud']\n",
    "# del X_test['is_fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.info())\n",
    "print('-' * 100)\n",
    "print(\"shape: \", X_train.shape)\n",
    "print('-' * 100)\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['first', 'last', 'trans_num', 'Unnamed: 0', 'street', 'trans_date_trans_time']\n",
    "X_train = X_train.drop(to_drop, axis=1)\n",
    "X_test = X_test.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMOUNT_OF_FRAUD = Y_train.value_counts()[1]\n",
    "print('No Frauds', round(Y_train.value_counts()[0]/len(Y_train) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(Y_train.value_counts()[1]/len(Y_train) * 100,2), '% of the dataset')\n",
    "print('Total amount of fraud transactions:', AMOUNT_OF_FRAUD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "\n",
    "sns.countplot('is_fraud', data=X_train, palette=colors)\n",
    "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax1 = plt.subplots(1, 1, figsize=(24,20))\n",
    "\n",
    "sns.heatmap(X_train.corr(), cmap='coolwarm_r', annot_kws={'size':10},annot=True, ax=ax1)\n",
    "ax1.set_title(\"Correlation Matrix)\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Looking on categorical data and try to identify which can be transformed to numeric categorical\")\n",
    "# print(\"trans_date_trans_time\", X_test['trans_date_trans_time'].nunique()) # too much, we need to modify it\n",
    "print(\"merchant\", X_test['merchant'].nunique())\n",
    "print(\"category\", X_test['category'].nunique())\n",
    "print(\"gender\", X_test['gender'].nunique())\n",
    "print(\"city\", X_test['city'].nunique())\n",
    "print(\"state\", X_test['state'].nunique())\n",
    "print(\"job\", X_test['job'].nunique())\n",
    "print(\"dob\", X_test['dob'].nunique())\n",
    "\n",
    "X_train.gender = X_train.gender.astype('category').cat.codes\n",
    "X_test.gender = X_test.gender.astype('category').cat.codes\n",
    "\n",
    "X_train.job = X_train.job.astype('category').cat.codes\n",
    "X_test.job = X_test.job.astype('category').cat.codes\n",
    "\n",
    "X_train.category = X_train.category.astype('category').cat.codes\n",
    "X_test.category = X_test.category.astype('category').cat.codes\n",
    "\n",
    "X_train.merchant = X_train.merchant.astype('category').cat.codes\n",
    "X_test.merchant = X_test.merchant.astype('category').cat.codes\n",
    "\n",
    "X_train.merchant = X_train.merchant.astype('category').cat.codes\n",
    "X_test.merchant = X_test.merchant.astype('category').cat.codes\n",
    "\n",
    "X_train.dob = X_train.dob.astype('category').cat.codes\n",
    "X_test.dob = X_test.dob.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = convert_cc_to_cc_type_and_drop(X_train)\n",
    "X_test = convert_cc_to_cc_type_and_drop(X_test)\n",
    "\n",
    "X_train.cc_type = X_train.cc_type.astype('category').cat.codes\n",
    "X_test.cc_type = X_test.cc_type.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = unite_state_city(X_train)\n",
    "X_test = unite_state_city(X_test)\n",
    "\n",
    "X_train.city = X_train.city.astype('category').cat.codes\n",
    "X_test.city = X_test.city.astype('category').cat.codes\n",
    "\n",
    "X_train.state = X_train.state.astype('category').cat.codes\n",
    "X_test.state = X_test.state.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()\n",
    "print(\"-\" * 100)\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax1 = plt.subplots(1, 1, figsize=(24,20))\n",
    "\n",
    "sns.heatmap(X_train.corr(), cmap='coolwarm_r', annot_kws={'size':10},annot=True, ax=ax1)\n",
    "ax1.set_title(\"Correlation Matrix)\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# del X_train['is_fraud']\n",
    "scores = cross_validate(LogisticRegression(),\n",
    "                        X_train, Y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(RandomForestClassifier(n_estimators=100,\n",
    "                                               class_weight='balanced'),\n",
    "                        X_train, Y_train, cv=10,\n",
    "                        scoring=('roc_auc', 'average_precision'))\n",
    "scores['test_roc_auc'].mean(), scores['test_average_precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# K-Nearest Neighbors is usually where the introduction class leaves off\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, Y_train)\n",
    "nb = BernoulliNB()\n",
    "nb.fit(X_train, Y_train)\n",
    "\n",
    "print(\"KNN\")\n",
    "print(classification_report(knn.predict(X_test), y_test))\n",
    "print(\"Naive Bayesian\")\n",
    "print(classification_report(nb.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = drop_columns(read_data(\"fraudTrain.csv\"), [\"Unnamed: 0\"])\n",
    "df = data\n",
    "# data = convert_cc_to_cc_type_and_drop(data)\n",
    "# data = convert_dob_to_age_and_drop(data)\n",
    "# data = unite_state_city(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = data[data['is_fraud']==1]\n",
    "normal = data[data['is_fraud']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_and_scale(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
    "\n",
    "amount_val = data['scaled_amount'].values\n",
    "time_val = data['trans_hour'].values\n",
    "\n",
    "sns.distplot(amount_val, ax=ax[0], color='r')\n",
    "ax[0].set_title('Distribution of Transaction Amount', fontsize=14)\n",
    "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
    "\n",
    "sns.distplot(time_val, ax=ax[1], color='b')\n",
    "ax[1].set_title('Distribution of Transaction Time', fontsize=14)\n",
    "ax[1].set_xlim([min(time_val), max(time_val)])\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create balanced data set ###\n",
    "\n",
    "df = data.sample(frac=1) #shuffled data (entire set)\n",
    "\n",
    "fraud_df = df.loc[df['is_fraud'] == 1]\n",
    "non_fraud_df = df.loc[df['is_fraud'] == 0][:AMOUNT_OF_FRAUD]\n",
    "\n",
    "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "\n",
    "#new_df.head()\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Distribution of the fraud in the subsample dataset')\n",
    "print(new_df['is_fraud'].value_counts()/len(new_df))\n",
    "\n",
    "\n",
    "\n",
    "sns.countplot('is_fraud', data=new_df, palette=colors)\n",
    "plt.title('Equally Distributed Classes', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n",
    "\n",
    "# Entire DataFrame\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':10},annot=True, ax=ax1)\n",
    "ax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n",
    "\n",
    "\n",
    "sub_sample_corr = new_df.corr()\n",
    "sns.heatmap(sub_sample_corr, cmap='coolwarm_r',  annot_kws={'size':10},annot=True, ax=ax2)\n",
    "ax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['gender','city','state','cc_type','age_in_days','merchant','amt','unix_time','trans_date_trans_time','first','last','street','trans_num','job','age','city_pop','zip','lat','long', 'merch_lat','merch_long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = drop_columns(new_df,columns_to_drop)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"RandomForest\" : RandomForestClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = new_df['is_fraud'].values\n",
    "converted_df_no_label = new_df.drop('is_fraud',axis=1).values\n",
    "converted_df_no_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(converted_df_no_label, label)\n",
    "    training_score = cross_val_score(classifier, converted_df_no_label, label, cv=5, scoring=\"f1\")\n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression \n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n",
    "grid_log_reg.fit(converted_df_no_label, label)\n",
    "# We automatically get the logistic regression with the best parameters.\n",
    "log_reg = grid_log_reg.best_estimator_\n",
    "\n",
    "print(\"log reg: best params\",grid_log_reg.best_params_)\n",
    "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\n",
    "grid_knears.fit(converted_df_no_label, label)\n",
    "# KNears best estimator\n",
    "knears_neighbors = grid_knears.best_estimator_\n",
    "print(\"grid_knears: best params\",grid_knears.best_params_)\n",
    "\n",
    "# Support Vector Classifier\n",
    "svc_params = {'C': [0.5, 0.7, 0.9, 1]} #,  'gamma': [1e-3, 1e-4]}\n",
    "#               'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
    "grid_svc = GridSearchCV(SVC(), svc_params)\n",
    "grid_svc.fit(converted_df_no_label, label)\n",
    "\n",
    "# SVC best estimator\n",
    "svc = grid_svc.best_estimator_\n",
    "print(\"svc: best params\",grid_svc.best_params_)\n",
    "\n",
    "# DecisionTree Classifier\n",
    "tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n",
    "              \"min_samples_leaf\": list(range(5,7,1))}\n",
    "grid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\n",
    "grid_tree.fit(converted_df_no_label, label)\n",
    "\n",
    "# tree best estimator\n",
    "tree_clf = grid_tree.best_estimator_\n",
    "print(\"tree: best params\",grid_tree.best_params_)\n",
    "\n",
    "# RandomForest\n",
    "forest_params = {\"criterion\": [\"gini\", \"entropy\"]}   #\"max_depth\": [list(range(2,4,1)), \n",
    "             # \"min_samples_leaf\": list(range(5,7,1))}\n",
    "grid_forest = GridSearchCV(RandomForestClassifier(), forest_params)\n",
    "grid_forest.fit(converted_df_no_label, label)\n",
    "\n",
    "# tree best estimator\n",
    "forest_clf = grid_forest.best_estimator_\n",
    "print(\"forest: best params\",grid_forest.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### f1 score ###\n",
    "log_reg_score = cross_val_score(log_reg, converted_df_no_label, label, cv=5, scoring=\"f1\")\n",
    "print('Logistic Regression Cross Validation f1 Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "knears_score = cross_val_score(knears_neighbors, converted_df_no_label, label, cv=5, scoring=\"f1\")\n",
    "print('Knears Neighbors Cross Validation f1 Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "svc_score = cross_val_score(svc, converted_df_no_label, label, cv=5, scoring=\"f1\")\n",
    "print('Support Vector Classifier Cross Validation f1 Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "tree_score = cross_val_score(tree_clf, converted_df_no_label, label, cv=5, scoring=\"f1\")\n",
    "print('DecisionTree Classifier Cross Validation f1 Score', round(tree_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "forest_score = cross_val_score(forest_clf, converted_df_no_label, label, cv=5,scoring=\"f1\")\n",
    "print('RandomForest Classifier Cross Validation f1 Score', round(forest_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df =  drop_columns(read_data(\"fraudTest.csv\"), [\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()\n",
    "y_test = test_df['is_fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = convert_and_scale(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df= drop_columns(test_df,columns_to_drop)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = drop_columns(test_df,['is_fraud'])\n",
    "\n",
    "forest_clf.fit(converted_df_no_label, label)\n",
    "predictions = forest_clf.predict(test_df)\n",
    "print(classification_report(y_test, predictions, target_names=['Normal','Is Fraud']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "print(f\"accuracy is {model.score(test_df, y_test)}\")\n",
    "plot_confusion_matrix(model, test_df, y_test, cmap='gray_r',normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([fraud.amt.describe(), normal.amt.describe()], keys=[\"fraud_amt\", \"non_fraud_amt\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot all the features via seaborn ### \n",
    "percent = 0.01\n",
    "columns_to_drop = ['trans_date_trans_time','state','zip','lat','long','first','last','street','trans_num','unix_time']\n",
    "partial_data = get_partial_data(data,percent)\n",
    "partial_data = drop_columns(partial_data,columns_to_drop)\n",
    "partial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(partial_data, hue='is_fraud') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### atm #####\n",
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "print()\n",
    "plt.subplot(2, 1, 1)\n",
    "subfraud = fraud #[fraud.amt<500]\n",
    "subfraud.amt.hist(bins=400,weights=np.ones(len(subfraud.amt)) / len(subfraud.amt))\n",
    "plt.title(\"Fraudulant Transaction Amount Distribution\")\n",
    "plt.legend()\n",
    "# plt.xlim([-10,400])\n",
    "# plt.ylim([0,0.1])\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "subnormal = normal #[normal.amt<400]\n",
    "subnormal.amt.hist(bins=500, weights=np.ones(len(subnormal.amt)) / len(subnormal.amt))\n",
    "plt.title(\"Normal Transaction Amount Distribution\")\n",
    "plt.legend()\n",
    "# plt.xlim([-10,400])\n",
    "# plt.ylim([0,0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We can see that transaction's amount is a good feature for fraud transaction recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### atm via seaborn ###\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.set_style('whitegrid')\n",
    "sns.catplot(x='is_fraud',y='amt', hue='is_fraud',data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### time #####\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "fraud_24time = fraud.copy()\n",
    "fraud_24time.unix_time = fraud_24time.unix_time % 86400 #(86400s per 24h)\n",
    "fraud_24time.unix_time /= (86400/24)\n",
    "fraud_24time.unix_time.hist(bins=50,label=\"Fraudulant Transaction\",weights=np.ones(len(fraud_24time)) / len(fraud_24time))\n",
    "plt.xlim([0,24])\n",
    "plt.ylim([0,0.15])\n",
    "plt.xlabel(\"\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "normal_24time = normal.copy()\n",
    "normal_24time.unix_time  = normal_24time.unix_time  % 86400 #(86400s per 24h)\n",
    "normal_24time.unix_time  /= (86400/24)\n",
    "normal_24time.unix_time .hist(bins=50,label=\"Non Fraudulant Transaction\",weights=np.ones(len(normal_24time)) / len(normal_24time))\n",
    "plt.xlim([0,24])\n",
    "plt.ylim([0,0.15])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We can see that transaction's time is a good feature for fraud transaction recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### category #####\n",
    "category_df = pd.concat([fraud.category.value_counts()/len(fraud.index), normal.category.value_counts()/len(normal.index)], keys=[\"fraud_category\", \"non_fraud_category\"], axis=1)\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20,10))\n",
    "\n",
    "category_df.plot(kind='bar',ax=axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We can see that transaction category is a good feature for fraud recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### city #####\n",
    "np.set_printoptions(suppress=True)\n",
    "dropped = data.drop_duplicates([\"city\"], inplace=False)\n",
    "total_pupulation = dropped.city_pop.sum()\n",
    "\n",
    "dropped.city_pop = dropped.city_pop / total_pupulation\n",
    "dropped.index  = dropped.city\n",
    "dropped\n",
    "\n",
    "city_df = pd.concat(\n",
    "    [\n",
    "        fraud.city.value_counts()/len(fraud.index), \n",
    "        normal.city.value_counts()/len(normal.index),\n",
    "        # dropped.city_pop\n",
    "    ], \n",
    "    # keys=[\"fraud\", \"normal\", \"normalized_pop\"], \n",
    "    keys=[\"fraud\", \"normal\"], \n",
    "    axis=1)\n",
    "\n",
    "city_df\n",
    "\n",
    "# /total_pupulation\n",
    "# dropped.city_pop.value_counts()\n",
    "\n",
    "# grouped = data.groupby('city')\n",
    "# data.city.unique()\n",
    "# data.loc['city', data.city.unique()]\n",
    "\n",
    "# data.city_pop.sum()\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20,10))\n",
    "city_df.plot(kind='bar',ax=axes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(20,10))\n",
    "axes.set_title(\"50 cities with high fraud distribution\")\n",
    "city_df[:50].plot(kind='bar',ax=axes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_df.index[0]\n",
    "city_count = {}\n",
    "for i in range(50):\n",
    "    fraud_city = fraud[fraud['city']==city_df.index[i]]\n",
    "    lat_unique = fraud_city.lat.nunique()\n",
    "    long_unique = fraud_city.long.nunique()\n",
    "    city_count[city_df.index[i]]= lat_unique > 1  or long_unique > 1\n",
    "\n",
    "city_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We can see that location of card holder is not a good feature to recognize fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 4, figsize=(20, 50))\n",
    "\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    fraud_city =  fraud[fraud['city']==city_df.index[i]]\n",
    "    normal_city =  normal[normal['city']==city_df.index[i]]\n",
    "    ax.scatter(normal_city.merch_lat, normal_city.merch_long, label=\"Normal\", alpha=1,color='green')\n",
    "    ax.scatter(fraud_city.merch_lat, fraud_city.merch_long,label='Fraud', alpha=1, color='red',s=400)\n",
    "    ax.legend()\n",
    "    feature_name = city_df.index[i]\n",
    "    ax.set_title(\"{}\".format(feature_name), fontdict=title_font)\n",
    "    ax.grid()\n",
    "    \n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Above we plot the locations of merchant in the cities with the highest fraud density. As we can see, there is no pattern and merchan location is not a good feature to recognize the fraud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### population ###\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "fraud.city_pop.hist(bins=50,label=\"Fraudulant City Population\",density=True,color='red')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "normal.city_pop.hist(bins=50,label=\"Non Fraudulant City Population\",density=True,color='green')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We can see that city population is not a good feature to distinguish between a fraud and normal transaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### zip code ### \n",
    "fraud.zip\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "fraud.zip.hist(bins=50,label=\"Fraudulant Zip\",density=True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "normal.zip.hist(bins=50,label=\"Non Fraudulant Transaction\",density=True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "orig_zip = data['zip']\n",
    "label = data['is_fraud']\n",
    "data_no_label = drop_columns(data,'is_fraud')\n",
    "te = TargetEncoder(cols='zip',return_df=True, drop_invariant = True).fit(data_no_label,label)\n",
    "transformed = te.transform(data_no_label)\n",
    "data['zip'] = transformed['zip'] # replace zip with fraud probability per zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_db = pd.concat([orig_zip,transformed['zip'],label], axis=1, keys=['orig_zip', 'fraud_probability','label'])\n",
    "zip_faud_prob = pd.DataFrame(zip_db.groupby('orig_zip')['fraud_probability'].mean())\n",
    "zip_faud_prob.sort_values(by='fraud_probability',ascending = False, inplace = True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20,10))\n",
    "axes.set_title(\"Fraud probability per zip-code\")\n",
    "zip_faud_prob.plot(kind='bar',ax=axes,logy=True,rot = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(20,10))\n",
    "axes.set_title(\"Zipcodes with the highest Fraud probability\")\n",
    "zip_faud_prob[:50].plot(kind='bar',ax=axes,rot = 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We can see that there are zipcodes with high probability of fraud, therefore zipcode is a good feature to distinguish between a fraud and normal transaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### state #####\n",
    "state_df = pd.concat(\n",
    "    [\n",
    "        fraud.state.value_counts()/len(fraud.index), \n",
    "        normal.state.value_counts()/len(normal.index),\n",
    "    ], \n",
    "    keys=[\"fraud\", \"normal\"], \n",
    "    axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(20,10))\n",
    "axes.set_title(\"Distribution of card holders per state\")\n",
    "state_df.plot(kind='bar',ax=axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### gender #####\n",
    "gender_df = pd.concat(\n",
    "    [\n",
    "        fraud.gender.value_counts()/len(fraud.index), \n",
    "        normal.gender.value_counts()/len(normal.index),\n",
    "    ], \n",
    "    keys=[\"fraud\", \"normal\"], \n",
    "    axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20,10))\n",
    "\n",
    "gender_df.plot(kind='bar',ax=axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As we can see, gender is not significally vary between fraud and normal card holders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### job #####\n",
    "job_db = pd.concat(\n",
    "    [\n",
    "        fraud.job.value_counts()/len(fraud.index), \n",
    "        normal.job.value_counts()/len(normal.index),\n",
    "    ], \n",
    "    keys=[\"fraud\", \"normal\"], \n",
    "    axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20,10))\n",
    "\n",
    "job_db.plot(kind='bar',ax=axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We can see that job is a good feature for fraud recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    r = 6371\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2-lat1)\n",
    "    delta_lambda = np.radians(lon2-lon1)\n",
    "    a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) *   np.sin(delta_lambda / 2)**2\n",
    "    res = r * (2 * np.arctan2(np.sqrt(a), np.sqrt(1-a)))\n",
    "    return np.round(res, 2)\n",
    "\n",
    "def calculate_holder_merch_distance(df):\n",
    "    _df = df.copy()\n",
    "    dist_list = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        dist = haversine_distance(row['lat'],row['long'],row['merch_lat'],row['merch_long'])\n",
    "        dist_list.append(dist)\n",
    "    _df['holder_merch_dist'] = dist_list\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### location ### \n",
    "\n",
    "fraud  = calculate_holder_merch_distance(fraud)\n",
    "normal = calculate_holder_merch_distance(normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "fraud.holder_merch_dist.hist(bins=50,label=\"Fraudulant Distance\",density=True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "normal.holder_merch_dist.hist(bins=50,label=\"Non Fraudulant Distance\",density=True)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We can see that distance between card holder location and merchant location is not a good feature for fraud recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### card type #####\n",
    "card_type_df = pd.concat([fraud.cc_type.value_counts()/len(fraud.index), normal.cc_type.value_counts()/len(normal.index)], keys=[\"fraud_category\", \"non_fraud_category\"], axis=1)\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20,10))\n",
    "card_type_df.plot(kind='bar',ax=axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### avrg amount per card type ### \n",
    "fraud_avrg_card_amt = fraud.groupby('cc_type')['amt'].mean()\n",
    "normal_avrg_card_amt = normal.groupby('cc_type')['amt'].mean()\n",
    "amt_per_category = pd.concat([fraud_avrg_card_amt, normal_avrg_card_amt], keys=[\"fraud_category\", \"non_fraud_category\"], axis=1)\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20,10))\n",
    "amt_per_category.plot(kind='bar',ax=axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We can see that card type is not a good feature for fraud recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### merchant #####\n",
    "merch_type_df = pd.concat([fraud.merchant.value_counts()/len(fraud.index), normal.merchant.value_counts()//len(normal.index)], keys=[\"fraud_category\", \"non_fraud_category\"], axis=1)\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20,10))\n",
    "axes.set_title(\"Merchant\")\n",
    "merch_type_df.plot(kind='bar',ax=axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 20\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20,10))\n",
    "axes.set_title(\"{} {} {}\".format(\"Top\", num, \"of fraud merchant\"))\n",
    "merch_type_df[:num].plot(kind='bar',ax=axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "We can see that there are merchants that has only fraud transactions. \n",
    "## TBD \n",
    "What we can conclude from this? Should we take into account specific merchant when we train the model? It's heavy feature - 693 unique categories. We have to OneHot it, means +693 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dob ###\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "fraud.age_in_days.hist(bins=100,label=\"Fraudulant Age\",density=True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "normal.age_in_days.hist(bins=100,label=\"Non Fraudulant Age\",density=True)\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We can see that age is not a good feature for fraud recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### average amt per category ### \n",
    "fraud_avrg_amt = fraud.groupby('category')['amt'].mean()\n",
    "normal_avrg_amt = normal.groupby('category')['amt'].mean()\n",
    "amt_per_category = pd.concat([fraud_avrg_amt, normal_avrg_amt], keys=[\"fraud_category\", \"non_fraud_category\"], axis=1)\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20,10))\n",
    "amt_per_category.plot(kind='bar',ax=axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We can see that there is significant difference between average amount of transactions made in each category between fraud and normal card holder. Also we can see that the most fraud transaction are made in the Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (14,12))\n",
    "plt.title('Correlation of Numeric Features',y=1,size=16)\n",
    "sns.heatmap(data.corr(),square = True,  vmax=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "\n",
    "print()\n",
    "plt.subplot(2, 1, 1)\n",
    "subfraud = fraud[fraud.amt<500]\n",
    "subfraud.amt.hist(bins=400,weights=np.ones(len(subfraud.amt)) / len(subfraud.amt))\n",
    "plt.title(\"Fraudulant Transaction Amount Distribution\")\n",
    "plt.legend()\n",
    "plt.xlim([-10,400])\n",
    "plt.ylim([0,0.1])\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "subnormal = normal[normal.amt<400]\n",
    "subnormal.amt.hist(bins=500, weights=np.ones(len(subnormal.amt)) / len(subnormal.amt))\n",
    "plt.title(\"Normal Transaction Amount Distribution\")\n",
    "plt.legend()\n",
    "plt.xlim([-10,400])\n",
    "plt.ylim([0,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = read_test_data()\n",
    "df_test = convert_to_categorical_trans_date_and_drop(df_test)\n",
    "# get_categorical_features(df_test).columns\n",
    "\n",
    "df_test.amt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(11, 1, figsize=(20, 70))\n",
    "continuous_features = get_continuous_features(df_test)\n",
    "\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    feature_name = continuous_features.columns[i]\n",
    "    feature_human_readable_name = features_description.get(feature_name, \"===NONE===\")\n",
    "    y = continuous_features[feature_name].value_counts().sort_index()\n",
    "    ax.set_title(\"{} / \\n {}\".format(feature_name, feature_human_readable_name), fontdict=title_font)\n",
    "    ax.set_ylabel(\"Counts\", fontdict=labels_font)\n",
    "    ax.plot(y)\n",
    "    ax.grid()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 3, figsize=(20, 70))\n",
    "x = df_test.is_fraud\n",
    "\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    X_col = continuous_features.iloc[:, i]\n",
    "#     ax.set_xlim(0, 1)\n",
    "    ax.set_xlim(right=1)\n",
    "    if X_col.nunique() > 10:\n",
    "        ax.set_xlim(X_col.quantile(q=.0001), X_col.quantile(q=.9999))\n",
    "        \n",
    "    ax.scatter(x, X_col, alpha=.5)\n",
    "\n",
    "    feature_name = continuous_features.columns[i]\n",
    "    feature_human_readable_name = features_description.get(feature_name, \"===NONE===\")\n",
    "    ax.set_title(\"{} / \\n {}\".format(feature_name, feature_human_readable_name), fontdict=title_font)\n",
    "#     ax.set_ylabel(\"Price ($)\", fontdict=labels_font)\n",
    "    ax.grid()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categorical_features = get_categorical_features(df_test)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(30,60))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    if(i >= len(categorical_features.columns)):\n",
    "        break\n",
    "    categorical_features.iloc[:,i].value_counts().plot(kind='bar',ax=ax,rot=45)\n",
    "    ax.set_title(categorical_features.columns[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Preprocessing and Baseline Model\n",
    "\n",
    "Create a simple minimum viable model by doing an initial selection of features, doing appropriate preprocessing and cross-validating a linear model. Feel free to generously exclude features or do simplified preprocessing for this task. As mentioned before, you dont need to validate the model on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pay attention: train data contains 1.300.000 samples\n",
    "percent = 0.2 # 20%\n",
    "partial_data = get_partial_data(data,percent)\n",
    "print (\"data size: {}\\npercent: {:.2f}\\npartial_data size {}\".format(len(data.index),percent,len(partial_data.index)))\n",
    "partial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colums_to_drop = ['cc_num','gender','first','last','street','trans_num','merch_lat','merch_long','dob','lat','long','city_pop','merchant','state','trans_date_trans_time','unix_time']\n",
    "partial_data= drop_columns(partial_data,colums_to_drop)\n",
    "partial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = partial_data.dtypes == object\n",
    "ohe = ColumnTransformer([('encoder', OneHotEncoder(), categorical)], remainder='passthrough')\n",
    "transformed = pd.DataFrame.sparse.from_spmatrix(ohe.fit_transform(partial_data),columns=ohe.get_feature_names()) #TBD fix names of columns to human-readable\n",
    "transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Feature Engineering\n",
    "Create derived features and perform more in-depth preprocessing and data cleaning. Does this improve your model? In particular, think about how to encode categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 Any model\n",
    "Use any classification model we discussed (trees, forests, boosting, SVM) to improve your result. You can (and probably should) change your preprocessing and feature engineering to be suitable for the model. You are not required to try all of these models. Tune parameters as appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 Feature Selections\n",
    "Identify features that are important for your best model. Which features are most influential,and which features could be removed without decrease in performance? Does removing irrelevant features make your model better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 An explainable model\n",
    "Can you create an explainable model that is nearly as good as your best model? An explainable model should be small enough to be easily inspected - say a linear model withfew enough coefficients that you can reasonable look at all of them, or a tree with a smallnumber of leafs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
